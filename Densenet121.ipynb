{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Densenet121.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUHBZc9fJJbQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAN6qacjOrP"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tup9MxxFVKYV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e877eb4-b4ba-40f9-ae47-5cf1180b7df2"
      },
      "source": [
        "from keras import backend as K \n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.models import Sequential\n",
        "from keras.models import Model \n",
        "from keras.layers import Conv2D, MaxPooling2D \n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.applications import InceptionResNetV2 \n",
        "! pip install split-folders\n",
        "import tensorflow as tf\n",
        "import split_folders\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "balFabPYVWn9"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import densenet\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDLfjG70WvgG"
      },
      "source": [
        "train_data_dir = \"/content/drive/My Drive/Neural_ProjectC2/augmented_images/train\"\n",
        "val_data_dir = \"/content/drive/My Drive/Neural_ProjectC2/augmented_images/validation\"\n",
        "num_train = 0\n",
        "num_val = 0\n",
        "\n",
        "for i in os.listdir(train_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(train_data_dir, i))\n",
        "  else:\n",
        "    num_train += len(os.listdir(os.path.join(train_data_dir, i)))\n",
        "print(\"train=\",num_train) \n",
        "\n",
        "for i in os.listdir(val_data_dir):\n",
        "  if i.startswith(\".\"):\n",
        "    os.rmdir(os.path.join(val_data_dir, i))\n",
        "  else:\n",
        "    num_val += len(os.listdir(os.path.join(val_data_dir, i)))\n",
        "print(\"val=\",num_val)\n",
        "\n",
        "img_width, img_height = 224, 224\n",
        "# Check if the images are RGB and change the channels likewise\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape= (3, img_width, img_height)\n",
        "else:\n",
        "  input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmT9U2z9XmZG"
      },
      "source": [
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense\n",
        "base_model =  DenseNet121(input_shape=(224, 224, 3),\n",
        "                                  weights='imagenet',\n",
        "                                  include_top = False,\n",
        "                                  pooling='avg')\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = Dense(1024)(x)\n",
        "\n",
        "\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(512)(x)\n",
        "\n",
        "\n",
        "x = Activation('relu')(x)\n",
        "predictions = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics = ['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkcEsEFWc622"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "def get_callbacks():\n",
        "\n",
        "   path_checkpoint ='checkpoint_keras'  \n",
        "   log_dir='logs'\n",
        "   \n",
        "   callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                         monitor='val_loss',\n",
        "                                         verbose=1,\n",
        "                                         save_weights_only=False,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max',\n",
        "                                         period=1)\n",
        "   \n",
        "   callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                           patience=5,\n",
        "                                           verbose=1)\n",
        "   \n",
        "   callback_tensorboard = TensorBoard(log_dir=log_dir,\n",
        "                                      histogram_freq=0,\n",
        "                                      write_graph=False)\n",
        "   \n",
        "   callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                          factor=0.1,\n",
        "                                          min_lr=1e-4,\n",
        "                                          patience=3,\n",
        "                                          verbose=1)\n",
        "\n",
        "   callbacks = [callback_checkpoint, callback_tensorboard, callback_reduce_lr]\n",
        "\n",
        "   return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBvfuYFdFpR"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    batch_size = batch_size, class_mode = 'categorical')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory( val_data_dir, \n",
        "                                                         target_size =(img_width, img_height), \n",
        "                                                         batch_size = batch_size, class_mode ='categorical') \n",
        "\n",
        "history=model.fit_generator(train_generator, \n",
        "                    steps_per_epoch = num_train // batch_size, \n",
        "                    epochs = epochs, validation_data = validation_generator, \n",
        "                    validation_steps = num_val// batch_size,\n",
        "                    workers=1,use_multiprocessing= False,callbacks = get_callbacks())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzKIR0L_W3Od"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/Neural_ProjectC2/models/model_dense121.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULeKq0hna9EP"
      },
      "source": [
        "import numpy\n",
        "import keras\n",
        "import sklearn.metrics as metrics\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 8\n",
        "epochs = 100\n",
        "test_set_dir = \"/content/drive/My Drive/Neural_ProjectC2/data_test\"\n",
        "\n",
        "num_test = len(os.listdir(test_set_dir))\n",
        "\n",
        "print (\"Number of images in test set: \", num_test)\n",
        "\n",
        "#model = get_model(0.0001)\n",
        "\n",
        "model = keras.models.load_model(\"/content/drive/My Drive/Neural_ProjectC2/models/model_dense121.h5\")\n",
        "  \n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_set_dir, target_size =(img_width, img_height), \n",
        "                                                  batch_size = batch_size, class_mode =None, shuffle = False) \n",
        "#print(test_datagen)\n",
        "test_steps_per_epoch = test_generator.n/test_generator.batch_size\n",
        "#print(test_generator.n)\n",
        "test_generator.reset()\n",
        "predictions = model.predict_generator(test_generator, steps = test_steps_per_epoch)\n",
        "\n",
        "print(\"PREDICTIONS: --->\")\n",
        "print(predictions)\n",
        "\n",
        "predicted_classes = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"PREDICTED CLASSES: --->\")\n",
        "print (predicted_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfnJXxDLvTRs"
      },
      "source": [
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odyu16COvmpj"
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# one hot encode\n",
        "encoded = to_categorical(predicted_classes)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo6x9pLdg9qx"
      },
      "source": [
        "np.savetxt(\"/content/drive/My Drive/Neural_ProjectC2/predict.csv\", encoded, delimiter=',',fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}