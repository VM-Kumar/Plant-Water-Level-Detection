{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgAN6qacjOrP"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tup9MxxFVKYV",
    "outputId": "7e877eb4-b4ba-40f9-ae47-5cf1180b7df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential\n",
    "from keras.models import Model \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.applications import InceptionResNetV2 \n",
    "#! pip install split-folders\n",
    "import tensorflow as tf\n",
    "import split_folders\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "balFabPYVWn9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import densenet\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gDLfjG70WvgG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-81ed80015f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#augmented training data source\n",
    "train_data_dir = \"/content/drive/My Drive/Neural_Project/augmented_images/train\"\n",
    "#augmented validation data source\n",
    "val_data_dir = \"/content/drive/My Drive/Neural_Project/augmented_images/validation\"\n",
    "num_train = 0\n",
    "num_val = 0\n",
    "\n",
    "for i in os.listdir(train_data_dir):\n",
    "  if i.startswith(\".\"):\n",
    "    os.rmdir(os.path.join(train_data_dir, i))\n",
    "  else:\n",
    "    num_train += len(os.listdir(os.path.join(train_data_dir, i)))\n",
    "print(\"train=\",num_train) \n",
    "\n",
    "for i in os.listdir(val_data_dir):\n",
    "  if i.startswith(\".\"):\n",
    "    os.rmdir(os.path.join(val_data_dir, i))\n",
    "  else:\n",
    "    num_val += len(os.listdir(os.path.join(val_data_dir, i)))\n",
    "print(\"val=\",num_val)\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "# Check if the images are RGB and change the channels likewise\n",
    "if K.image_data_format() == 'channels_first':\n",
    "  input_shape= (3, img_width, img_height)\n",
    "else:\n",
    "  input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmT9U2z9XmZG"
   },
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense\n",
    "base_model =  DenseNet121(input_shape=(224, 224, 3),\n",
    "                                  weights='imagenet',\n",
    "                                  include_top = False,\n",
    "                                  pooling='avg')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Dense(1024)(x)\n",
    "\n",
    "\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(512)(x)\n",
    "\n",
    "\n",
    "x = Activation('relu')(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(), metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkcEsEFWc622"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "def get_callbacks():\n",
    "\n",
    "   path_checkpoint ='checkpoint_keras'  \n",
    "   log_dir='logs'\n",
    "   \n",
    "   callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                         monitor='val_loss',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=True,\n",
    "                                         mode='max',\n",
    "                                         period=1)\n",
    "   \n",
    "   callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                           patience=5,\n",
    "                                           verbose=1)\n",
    "   \n",
    "   callback_tensorboard = TensorBoard(log_dir=log_dir,\n",
    "                                      histogram_freq=0,\n",
    "                                      write_graph=False)\n",
    "   \n",
    "   callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                          factor=0.1,\n",
    "                                          min_lr=1e-4,\n",
    "                                          patience=3,\n",
    "                                          verbose=1)\n",
    "\n",
    "   callbacks = [callback_checkpoint, callback_tensorboard, callback_reduce_lr]\n",
    "\n",
    "   return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcBvfuYFdFpR"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
    "                                                    target_size =(img_width, img_height), \n",
    "                                                    batch_size = batch_size, class_mode = 'categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory( val_data_dir, \n",
    "                                                         target_size =(img_width, img_height), \n",
    "                                                         batch_size = batch_size, class_mode ='categorical') \n",
    "\n",
    "history=model.fit_generator(train_generator, \n",
    "                    steps_per_epoch = num_train // batch_size, \n",
    "                    epochs = epochs, validation_data = validation_generator, \n",
    "                    validation_steps = num_val// batch_size,\n",
    "                    workers=1,use_multiprocessing= False,callbacks = get_callbacks())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzKIR0L_W3Od"
   },
   "outputs": [],
   "source": [
    "#path to store weights\n",
    "model.save(\"/content/drive/My Drive/Neural_Project/models/model_dense121.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULeKq0hna9EP"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "import sklearn.metrics as metrics\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 8\n",
    "epochs = 100\n",
    "\n",
    "#test images source\n",
    "test_set_dir = \"/content/drive/My Drive/Neural_Project/data_test\"\n",
    "\n",
    "num_test = len(os.listdir(test_set_dir))\n",
    "\n",
    "print (\"Number of images in test set: \", num_test)\n",
    "\n",
    "#model = get_model(0.0001)\n",
    "\n",
    "model = keras.models.load_model(\"/content/drive/My Drive/Neural_ProjectC2/models/model_dense121.h5\")\n",
    "  \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_set_dir, target_size =(img_width, img_height), \n",
    "                                                  batch_size = batch_size, class_mode =None, shuffle = False) \n",
    "#print(test_datagen)\n",
    "test_steps_per_epoch = test_generator.n/test_generator.batch_size\n",
    "#print(test_generator.n)\n",
    "test_generator.reset()\n",
    "predictions = model.predict_generator(test_generator, steps = test_steps_per_epoch)\n",
    "\n",
    "print(\"PREDICTIONS: --->\")\n",
    "print(predictions)\n",
    "\n",
    "predicted_classes = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"PREDICTED CLASSES: --->\")\n",
    "print (predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfnJXxDLvTRs"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Odyu16COvmpj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode\n",
    "encoded = to_categorical(predicted_classes)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo6x9pLdg9qx"
   },
   "outputs": [],
   "source": [
    "# path to store predictions\n",
    "np.savetxt(\"/content/drive/My Drive/Neural_Project/predict.csv\", encoded, delimiter=',',fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Densenet121.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
